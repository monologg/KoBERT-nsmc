# KoBERT-nsmc

- KoBERTë¥¼ ì´ìš©í•œ ë„¤ì´ë²„ ì˜í™” ë¦¬ë·° ê°ì • ë¶„ì„ (sentiment classification)
- ğŸ¤—`Huggingface Tranformers`ğŸ¤— ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì´ìš©í•˜ì—¬ êµ¬í˜„

## Dependencies

- torch==1.4.0
- transformers==2.10.0

## How to use KoBERT on Huggingface Transformers Library

- ê¸°ì¡´ì˜ KoBERTë¥¼ transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ê³§ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ë§ì·„ìŠµë‹ˆë‹¤.
  - transformers v2.2.2ë¶€í„° ê°œì¸ì´ ë§Œë“  ëª¨ë¸ì„ transformersë¥¼ í†µí•´ ì§ì ‘ ì—…ë¡œë“œ/ë‹¤ìš´ë¡œë“œí•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
- Tokenizerë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ `tokenization_kobert.py`ì—ì„œ `KoBertTokenizer`ë¥¼ ì„í¬íŠ¸í•´ì•¼ í•©ë‹ˆë‹¤.

```python
from transformers import BertModel
from tokenization_kobert import KoBertTokenizer

model = BertModel.from_pretrained('monologg/kobert')
tokenizer = KoBertTokenizer.from_pretrained('monologg/kobert')
```

## Usage

```bash
$ python3 main.py --model_type kobert --do_train --do_eval
```

## Prediction

```bash
$ python3 predict.py --input_file {INPUT_FILE_PATH} --output_file {OUTPUT_FILE_PATH} --model_dir {SAVED_CKPT_PATH}
```

## Results

|                   | Accuracy (%) |
| ----------------- | ------------ |
| KoBERT            | **89.63**    |
| DistilKoBERT      | 88.41        |
| Bert-Multilingual | 87.07        |
| FastText          | 85.50        |

## References

- [KoBERT](https://github.com/SKTBrain/KoBERT)
- [Huggingface Transformers](https://github.com/huggingface/transformers)
- [NSMC dataset](https://github.com/e9t/nsmc)
